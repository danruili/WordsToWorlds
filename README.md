# From Words to Worlds: Transforming One-line Prompts into Multi-modal Digital Stories with LLM Agents

Danrui Li [1*], Sam S. Sohn [1*], Sen Zhang [1*] , Che-Jui Chang [1], Mubbasir Kapadia [1,2]

[1] Rutgers University,
[*] Equal contribution,
[2] Roblox,

![Teaser](static\images\teaser.png)

Our work utilizes Large Language Models and generative tools to automate and refine digital storytelling.

Employing a top-down story drafting and bottom-up asset generation approach, this framework tackles key issues such as manual intervention, interactive scene orchestration, and narrative consistency. This framework enables efficient production of interactive and consistent digital storytellings across multiple modalities, democratizing content creation and enhancing engagement.

Read full paper on [MIG 2024](https://dl.acm.org/doi/10.1145/3677388.3696321).
For more technical details, please refer to our [project page](https://danruili.github.io/wordstoworlds/).

## Code Repository

We are preparing the codebase for release. Once it is ready, we will update the repository with the code and data.


## Cite our work
```bibtex
@inproceedings{words2worldsLi2024,
    author = {Li, Danrui and Sohn, Samuel S. and Zhang, Sen and Chang, Che-Jui and Kapadia, Mubbasir},
    title = {From Words to Worlds: Transforming One-line Prompts into Multi-modal Digital Stories with LLM Agents},
    year = {2024},
    isbn = {9798400710902},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3677388.3696321},
    doi = {10.1145/3677388.3696321},
    booktitle = {Proceedings of the 17th ACM SIGGRAPH Conference on Motion, Interaction, and Games},
    articleno = {21},
    numpages = {12},
    keywords = {Communicative Agents, Digital storytelling, Large Language Models},
    location = {Arlington, VA, USA},
    series = {MIG '24}
}

```